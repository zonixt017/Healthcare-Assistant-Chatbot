# HealthCare ChatBot üßëüèΩ‚Äç‚öïÔ∏è

An AI-powered medical chatbot that uses a Retrieval-Augmented Generation (RAG) architecture to answer questions from a medical PDF document. The entire application runs locally on your machine.

## üìú About The Project

This repository contains a medical chatbot application built to provide accurate, context-aware answers from a specific medical PDF document. It leverages a local Large Language Model (LLM) and a vector database to create a powerful, private, and efficient question-answering system.

The core idea is to provide an external knowledge source to the LLM when it answers a question, rather than relying solely on the model's pre-trained knowledge. This makes the answers more accurate and specific to the provided data.

### Core Features

*   **AI-Powered Medical Chatbot:** A conversational AI to answer questions about a medical knowledge base.
*   **Retrieval-Augmented Generation (RAG):** The chatbot uses a RAG pipeline for accurate, context-aware answers.
*   **Local & Private:** Utilizes a local Llama 2 LLM, ensuring data privacy and offline functionality.
*   **Semantic Search:** Implements semantic search over a PDF document to find the most relevant information.
*   **Interactive UI:** Built with Streamlit, featuring chat history and conversational memory for follow-up questions.

## ‚öôÔ∏è How It Works (Architecture)

The application is a self-contained Retrieval-Augmented Generation (RAG) system. Here is a detailed breakdown of each component:

1.  **User Interface (Streamlit):**
    The `app.py` script uses Streamlit to create the chatbot interface you interact with. It handles displaying the title, rendering the chat history, and providing an input box for you to type your questions.

2.  **Orchestration Framework (LangChain):**
    LangChain is the glue that holds the entire application together. It manages the flow of data from the initial PDF document to the final answer generated by the Llama 2 model.

3.  **Data Ingestion and Processing:**
    *   **PyPDFLoader:** The application reads the `data/MEDICAL.pdf` file using `pypdf`. LangChain's `DirectoryLoader` is configured to load PDF files from the `data/` directory.
    *   **RecursiveCharacterTextSplitter:** Since LLMs have a limited context window, the PDF is broken down into smaller, overlapping chunks to ensure semantic meaning isn't lost.

4.  **Semantic Understanding and Retrieval:**
    *   **HuggingFaceEmbeddings:** The application uses the `sentence-transformers/all-MiniLM-L6-v2` model to create "embeddings"‚Äînumerical vector representations of the text chunks. This runs locally on your CPU.
    *   **FAISS Vector Store:** The embeddings are loaded into a FAISS (Facebook AI Similarity Search) vector store. When you ask a question, FAISS compares your question's vector to all the chunk vectors to retrieve the most semantically similar text chunks. This is the "Retrieval" part of RAG.

5.  **The "Brain": The Large Language Model (LLM):**
    *   **Llama 2:** The project uses a 7-billion parameter `llama-2-7b-chat.ggmlv3.q4_0.bin` model. This quantized model is designed to run efficiently on CPUs without requiring a powerful GPU.
    *   **CTransformers:** This library allows the Python script to load and run the Llama 2 model directly on your CPU.

6.  **The Final Chain (ConversationalRetrievalChain):**
    This LangChain component ties everything together. It takes your question, considers the conversation history, retrieves relevant context from the FAISS vector store, builds a detailed prompt, and sends it to the Llama 2 model to generate a response.

## üõ†Ô∏è Technology Stack

*   **Python**
*   **Streamlit:** For the web-based user interface.
*   **LangChain:** As the core framework for building the RAG pipeline.
*   **Hugging Face Transformers:** For embeddings and model interaction.
*   **Llama 2:** A 7-billion parameter LLM for generating answers.
*   **FAISS:** For efficient similarity search in the vector store.
*   **CTransformers:** To run the quantized LLM on a CPU.

## üöÄ Getting Started

Follow these instructions to set up and run the project on your local machine.

### Prerequisites

*   Python 3.8 or higher
*   Git (for cloning the repository)

### Installation & Setup

1.  **Clone the repository:**
    ```sh
    git clone https://github.com/<your-username>/LLama2HealthCareChatBot.git
    cd LLama2HealthCareChatBot
    ```

2.  **Create and activate a virtual environment:**
    ```sh
    # For Windows
    python -m venv env
    .\env\Scripts\activate

    # For macOS/Linux
    python3 -m venv env
    source env/bin/activate
    ```

3.  **Install dependencies:**
    ```sh
    pip install -r requirements.txt
    ```

4.  **Download the LLM Model:**
    Download the `llama-2-7b-chat.ggmlv3.q4_0.bin` model file from the following link and place it in the root directory of the project.
    *   **Model:** `llama-2-7b-chat.ggmlv3.q4_0.bin`
    *   **Download Link:** [TheBloke/Llama-2-7B-Chat-GGML](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin)

5.  **Add Your Data:**
    Place the PDF document you want to chat with inside the `data/` directory. The application is currently configured to use `MEDICAL.pdf`.

## ‚ñ∂Ô∏è Usage

Once the setup is complete, you can run the Streamlit application with the following command:

```sh
streamlit run app.py
```

This will open a new tab in your web browser with the HealthCare ChatBot interface.

## üìÇ Project Structure

```
.
‚îú‚îÄ‚îÄ‚îÄ app.py             # Main Streamlit application file
‚îú‚îÄ‚îÄ‚îÄ data/
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ MEDICAL.pdf    # Knowledge base for the chatbot
‚îú‚îÄ‚îÄ‚îÄ llama-2-7b-chat.ggmlv3.q4_0.bin  # The LLM model file
‚îú‚îÄ‚îÄ‚îÄ requirements.txt   # Python dependencies
‚îî‚îÄ‚îÄ‚îÄ README.md          # This file
```
# ─────────────────────────────────────────────────────────────────────────────
# .env.example  –  Copy this file to ".env" and fill in your values.
#                  Never commit your real ".env" to version control.
# ─────────────────────────────────────────────────────────────────────────────

# ═══════════════════════════════════════════════════════════════════════════════
# LLM CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

# ── HuggingFace Cloud API (optional) ──────────────────────────────────────────
# Get your free token at https://huggingface.co/settings/tokens
# If set and working, the app will use cloud inference (fast, no local GPU needed).
# If blank or unavailable, falls back to local GGUF model.
HUGGINGFACEHUB_API_TOKEN=

# You can also use HF_TOKEN or HUGGINGFACE_API_TOKEN on some platforms
# (the app checks all three names).

# HuggingFace model ID for cloud inference
HF_INFERENCE_API=mistralai/Mistral-7B-Instruct-v0.2

# Preferred HF task (many instruct/chat models are "conversational")
HF_INFERENCE_TASK=conversational

# Optional fallback tasks (tried if provider rejects preferred task)
HF_INFERENCE_TASK_FALLBACKS=text-generation,conversational

# Optional comma-separated fallback model IDs (tried in order if primary fails)
HF_INFERENCE_FALLBACKS=meta-llama/Llama-3.1-8B-Instruct,Qwen/Qwen2.5-7B-Instruct

# Timeout (seconds) for Hugging Face Inference API request
HF_API_TIMEOUT=45

# ── Local GGUF Model (fallback / primary if no cloud token) ───────────────────
# Path to a local GGUF model file relative to project root.
# For free cloud demos (e.g., Hugging Face Spaces), you can leave LOCAL_LLM_PATH as-is
# without uploading a model, as long as HUGGINGFACEHUB_API_TOKEN is set.
# Recommended for GTX 1650 (4 GB VRAM):
#   • phi-2.Q4_K_M.gguf        — best quality/speed (~1.7 GB, all 32 layers on GPU)
#   • tinyllama-1.1b.Q4_K_M.gguf — fastest (~0.7 GB)
#   • mistral-7b-v0.1.Q4_K_M.gguf — best quality, tight fit (~4.1 GB, 20-24 GPU layers)
LOCAL_LLM_PATH=models/phi-2.Q4_K_M.gguf

# ── GPU Offloading ────────────────────────────────────────────────────────────
# Number of transformer layers to offload to GPU.
#   • Phi-2 (2.7B Q4):        32  (all layers fit in 4 GB VRAM)
#   • TinyLlama (1.1B Q4):    22  (all layers)
#   • Mistral-7B (Q4):        20-24 (partial offload, rest on CPU)
#   • CPU-only:               0
N_GPU_LAYERS=32

# ═══════════════════════════════════════════════════════════════════════════════
# DATA & VECTOR STORE
# ═══════════════════════════════════════════════════════════════════════════════

# Directory containing your PDF knowledge-base files
PDF_DATA_PATH=data/

# Directory where the FAISS vector index will be saved / loaded from
VECTOR_STORE_PATH=vectorstore

# ═══════════════════════════════════════════════════════════════════════════════
# EMBEDDINGS
# ═══════════════════════════════════════════════════════════════════════════════

# Sentence-Transformers model for text embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Device for embeddings: "cuda" (GPU) or "cpu"
# Leave blank to auto-detect (uses GPU if available)
EMBED_DEVICE=

# ═══════════════════════════════════════════════════════════════════════════════
# RETRIEVER
# ═══════════════════════════════════════════════════════════════════════════════

# Number of document chunks to retrieve per query (MMR search)
# Lower = faster but less context; higher = more context but slower
RETRIEVER_K=3
